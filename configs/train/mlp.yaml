experiment: 'MLP-iter02-exp01-10h-1f'
description: 'MLP baseline'
batch_size: 32
learning_rate: 1e-3
max_epochs: 10

train_params:
  learning_rate: 1e-3
  sched_lr_gamma: 0.1
  sched_lr_step: 3

resume_from_checkpoint: null

logging_cfg:
  path: 'logs'
  log_every_n_steps: 1

checkpoint_cfg:
  metric_monitor: 'val/loss'
  resume_from: null